#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraper pour rÃ©cupÃ©rer les donnÃ©es du Top Singles de SNEP Musique
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
import os
from urllib.parse import urljoin
import logging
from datetime import datetime
import re
import urllib3

# DÃ©sactiver les avertissements SSL non sÃ©curisÃ©s
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- Fonctions utilitaires de parsing (extraites pour modularitÃ© et tests) ---

def parse_artists_in_feat(artistes_text):
    """
    Parse les artistes dans un feat. (peut contenir &, virgules, etc.)
    """
    if not artistes_text:
        return []
    
    # SÃ©parer par & et virgules
    artistes = re.split(r'\s*[&,]\s*', artistes_text)
    return [a.strip() for a in artistes if a.strip()]

def handle_x_separator(text):
    """
    GÃ¨re intelligemment le X comme sÃ©parateur d'artistes
    """
    # Pattern pour dÃ©tecter un X entourÃ© d'espaces entre des mots qui ressemblent Ã  des noms
    # On cherche : [Mot(s)] X [Mot(s)] oÃ¹ les mots commencent par une majuscule
    x_pattern = r'\b([A-Z][A-Za-z\s]+?)\s+X\s+([A-Z][A-Za-z\s]+?)\b'
    
    def replace_x(match):
        artist1 = match.group(1).strip()
        artist2 = match.group(2).strip()
        
        # VÃ©rifications supplÃ©mentaires pour s'assurer que c'est bien des noms d'artistes
        # On Ã©vite de remplacer si les mots sont trop courts ou contiennent des caractÃ¨res suspects
        if (len(artist1) >= 2 and len(artist2) >= 2 and 
            not re.search(r'\d{3,}', artist1 + artist2) and  # Ã‰viter les nombres longs
            not re.search(r'\b(THE|AND|OF|FOR|WITH|IN|ON|AT)\b', artist1 + " " + artist2, re.IGNORECASE)):
            return f"{artist1}|SEPARATOR|{artist2}"
        else:
            # Retourner le texte original si Ã§a ne ressemble pas Ã  des noms d'artistes
            return match.group(0)
    
    return re.sub(x_pattern, replace_x, text)

def parse_artists(artiste_string):
    """
    SÃ©pare les artistes multiples selon les dÃ©limiteurs : virgule, FEAT., &, X (intelligent)
    
    Args:
        artiste_string: String contenant potentiellement plusieurs artistes
        
    Returns:
        Dict avec artiste, artiste_2, artiste_3, artiste_4
    """
    result = {
        'artiste': '',
        'artiste_2': '',
        'artiste_3': '',
        'artiste_4': ''
    }
    
    if not artiste_string or artiste_string.strip() == '':
        return result
    
    # Nettoyer la chaÃ®ne
    cleaned_string = artiste_string.strip()
    
    # Remplacer les diffÃ©rents sÃ©parateurs par un sÃ©parateur uniforme
    # On utilise |SEPARATOR| comme dÃ©limiteur temporaire unique
    separators = [
        (' FT. ', '|SEPARATOR|')
        (' FT ', '|SEPARATOR|'),
        (' ft ', '|SEPARATOR|'),
        ('FEAT.', '|SEPARATOR|'),
        ('FEAT', '|SEPARATOR|'),
        ('feat.', '|SEPARATOR|'),
        ('feat', '|SEPARATOR|'),
        ('Feat.', '|SEPARATOR|'),
        ('Feat', '|SEPARATOR|'),
        ('&', '|SEPARATOR|'),
        (',', '|SEPARATOR|')
    ]
    
    for old, new in separators:
        cleaned_string = cleaned_string.replace(old, new)
    
    # Gestion intelligente du X comme sÃ©parateur
    cleaned_string = handle_x_separator(cleaned_string)
    
    # SÃ©parer selon le dÃ©limiteur uniforme
    artists = [artist.strip() for artist in cleaned_string.split('|SEPARATOR|') if artist.strip()]
    
    # Assigner aux colonnes
    keys = ['artiste', 'artiste_2', 'artiste_3', 'artiste_4']
    for i, artist in enumerate(artists[:4]):  # Maximum 4 artistes
        if i < len(keys):
            result[keys[i]] = artist
    
    return result

def clean_title_and_extract_feat(titre):
    """
    Nettoie le titre en supprimant les parenthÃ¨ses et extrait les artistes en feat.
    
    Args:
        titre: Titre original
        
    Returns:
        Tuple (titre_propre, liste_artistes_feat)
    """
    if not titre:
        return titre, []
    
    titre_propre = titre.strip()
    artistes_feat = []
    
    # Chercher les contenus entre parenthÃ¨ses
    parentheses_pattern = r'\(([^)]+)\)'
    matches = re.findall(parentheses_pattern, titre_propre)
    
    for match in matches:
        # VÃ©rifier si c'est un feat.
        if re.search(r'\b(feat\.?|ft\.?|featuring)\b', match, re.IGNORECASE):
            # Extraire les artistes aprÃ¨s feat.
            feat_pattern = r'\b(?:feat\.?|ft\.?|featuring)\s+(.+)'
            feat_match = re.search(feat_pattern, match, re.IGNORECASE)
            if feat_match:
                artistes_text = feat_match.group(1).strip()
                # SÃ©parer les artistes dans le feat.
                artistes_dans_feat = parse_artists_in_feat(artistes_text)
                artistes_feat.extend(artistes_dans_feat)
    
    # Supprimer toutes les parenthÃ¨ses du titre
    titre_propre = re.sub(r'\s*\([^)]*\)\s*', ' ', titre_propre)
    titre_propre = re.sub(r'\s+', ' ', titre_propre).strip()
    
    return titre_propre, artistes_feat

def merge_artists(artists_data, feat_artists):
    """
    Fusionne les artistes principaux avec les artistes feat. sans doublon
    """
    # Collecter tous les artistes existants
    existing_artists = []
    for key in ['artiste', 'artiste_2', 'artiste_3', 'artiste_4']:
        if artists_data[key]:
            existing_artists.append(artists_data[key].upper())
    
    # Ajouter les artistes feat. s'ils ne sont pas dÃ©jÃ  prÃ©sents
    keys = ['artiste', 'artiste_2', 'artiste_3', 'artiste_4']
    for feat_artist in feat_artists:
        if feat_artist.upper() not in existing_artists:
            # Trouver la prochaine colonne vide
            for key in keys:
                if not artists_data[key]:
                    artists_data[key] = feat_artist
                    existing_artists.append(feat_artist.upper())
                    break
    
    return artists_data

# --- Fin des fonctions utilitaires ---

class SNEPScraper:
    def __init__(self, delay_between_requests=1.5):
        """
        Initialise le scraper SNEP
        
        Args:
            delay_between_requests: DÃ©lai en secondes entre chaque requÃªte
        """
        self.base_url = "https://snepmusique.com/les-tops/le-top-de-la-semaine/top-albums/"
        self.delay = delay_between_requests
        self.session = requests.Session()
        # DÃ©sactiver la vÃ©rification SSL pour Ã©viter les erreurs de certificats locaux
        self.session.verify = False
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # CrÃ©er le dossier data s'il n'existe pas
        self.data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            logger.info(f"Dossier '{self.data_dir}' crÃ©Ã©")
    
    def clean_title_and_extract_feat(self, titre):
        return clean_title_and_extract_feat(titre)
    
    def parse_artists_in_feat(self, artistes_text):
        return parse_artists_in_feat(artistes_text)
    
    def parse_artists(self, artiste_string):
        return parse_artists(artiste_string)
    
    def handle_x_separator(self, text):
        return handle_x_separator(text)
    
    def merge_artists(self, artists_data, feat_artists):
        return merge_artists(artists_data, feat_artists)
    
    def get_page_content(self, semaine, annee):
        """
        RÃ©cupÃ¨re le contenu HTML d'une page pour une semaine donnÃ©e
        
        Args:
            semaine: NumÃ©ro de la semaine
            annee: AnnÃ©e
            
        Returns:
            BeautifulSoup object ou None si erreur
        """
        params = {
            'categorie': 'Top Singles',
            'semaine': str(semaine),
            'annee': str(annee)
        }
        
        try:
            logger.info(f"RÃ©cupÃ©ration des donnÃ©es : AnnÃ©e {annee}, Semaine {semaine}")
            response = self.session.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la rÃ©cupÃ©ration de la page (AnnÃ©e {annee}, Semaine {semaine}): {e}")
            return None
    
    def extract_data_from_page(self, soup, semaine, annee):
        """
        Extrait les donnÃ©es de la page
        
        Args:
            soup: BeautifulSoup object
            semaine: NumÃ©ro de la semaine
            annee: AnnÃ©e
            
        Returns:
            Liste de dictionnaires contenant les donnÃ©es
        """
        data = []
        
        if not soup:
            return data
        
        try:
            # Chercher le conteneur principal avec les articles
            # Les donnÃ©es peuvent Ãªtre dans des articles ou des divs avec la classe 'item'
            items = soup.find_all('article', class_='classement-item')
            
            if not items:
                # Alternative : chercher des divs avec classe item
                items = soup.find_all('div', class_='item')
            
            if not items:
                # Autre alternative : chercher dans la structure div.items
                items_container = soup.find('div', class_='items')
                if items_container:
                    items = items_container.find_all(['article', 'div'], recursive=False)
            
            if not items:
                # DerniÃ¨re tentative : chercher toutes les structures qui ressemblent Ã  des items de classement
                main_content = soup.find(['main', 'div'], id=['primary', 'content', 'main-content'])
                if main_content:
                    # Chercher les blocs de donnÃ©es
                    items = []
                    
                    # Pattern pour identifier les blocs de classement
                    classement_blocks = main_content.find_all(['div', 'article'], 
                                                             class_=re.compile(r'(item|single|track|classement)', re.I))
                    
                    for block in classement_blocks:
                        # VÃ©rifier si c'est bien un item du classement
                        if block.find(text=re.compile(r'^\d+$|^\d+e?La Semaine', re.I)):
                            items.append(block)
            
            logger.info(f"Nombre d'items trouvÃ©s : {len(items)}")
            
            for item in items:
                try:
                    item_data = {}
                    
                    # Extraire le classement (nombre au dÃ©but ou dans une balise spÃ©cifique)
                    classement = None
                    
                    # PRIORITÃ‰ 1: Chercher spÃ©cifiquement la classe "rang" (SNEP standard)
                    # On exclut explicitement "rang_precedent"
                    classement_elem = item.find('div', class_='rang')
                    if classement_elem:
                        classement_text = classement_elem.get_text(strip=True)
                        match = re.search(r'(\d+)', classement_text)
                        if match:
                            classement = match.group(1)

                    # PRIORITÃ‰ 2: Si pas trouvÃ©, chercher avec des regex mais en excluant "precedent"
                    if not classement:
                        # On cherche les classes qui matchent rank/position/etc...
                        candidates = item.find_all(['span', 'div', 'strong'], class_=re.compile(r'(rank|position|classement|number)', re.I))
                        
                        for candidate in candidates:
                            # VÃ©rifier que la classe ne contient pas "precedent" ou "previous"
                            classes = candidate.get('class', [])
                            class_str = " ".join(classes).lower()
                            
                            if 'precedent' in class_str or 'previous' in class_str or 'last' in class_str:
                                continue
                                
                            classement_text = candidate.get_text(strip=True)
                            match = re.search(r'(\d+)', classement_text)
                            if match:
                                classement = match.group(1)
                                break
                    
                    if not classement:
                        # Chercher dans le texte de l'item (fallback)
                        text = item.get_text(strip=True)
                        match = re.match(r'^(\d+)', text)
                        if match:
                            classement = match.group(1)
                    
                    
                    # Extraire le titre, l'artiste et l'Ã©diteur
                    # Ces informations peuvent Ãªtre dans diffÃ©rentes balises
                    
                    # MÃ©thode 1: Chercher des balises spÃ©cifiques
                    titre_elem = item.find(['h2', 'h3', 'h4', 'h5', 'span', 'div'], 
                                          class_=re.compile(r'(title|titre|song|track)', re.I))
                    artiste_elem = item.find(['span', 'div', 'p'], 
                                            class_=re.compile(r'(artist|artiste|performer)', re.I))
                    editeur_elem = item.find(['span', 'div', 'p'], 
                                            class_=re.compile(r'(label|editeur|publisher|producer)', re.I))
                    
                    titre = titre_elem.get_text(strip=True) if titre_elem else None
                    artiste = artiste_elem.get_text(strip=True) if artiste_elem else None
                    editeur = editeur_elem.get_text(strip=True) if editeur_elem else None
                    
                    # MÃ©thode 2: Si pas trouvÃ©, essayer d'analyser le texte complet
                    if not all([titre, artiste, editeur]):
                        # Obtenir tout le texte et le diviser intelligemment
                        lines = []
                        for elem in item.find_all(text=True):
                            text = elem.strip()
                            if text and not re.match(r'^(\d+e?La Semaine|Nouveau)', text, re.I):
                                lines.append(text)
                        
                        # Filtrer les lignes pour enlever le classement et les infos de semaine
                        filtered_lines = []
                        for line in lines:
                            if not re.match(r'^\d+$', line) and len(line) > 2:
                                filtered_lines.append(line)
                        
                        # GÃ©nÃ©ralement : Titre, Artiste, Ã‰diteur
                        if len(filtered_lines) >= 3:
                            titre = titre or filtered_lines[0]
                            artiste = artiste or filtered_lines[1]
                            editeur = editeur or filtered_lines[2]
                        elif len(filtered_lines) == 2:
                            titre = titre or filtered_lines[0]
                            artiste = artiste or filtered_lines[1]
                        elif len(filtered_lines) == 1:
                            titre = titre or filtered_lines[0]
                    
                    # Si on a au moins le classement et le titre, ajouter l'entrÃ©e
                    if classement and titre:
                        # Nettoyer le titre et extraire les feat.
                        titre_propre, feat_artists = self.clean_title_and_extract_feat(titre)
                        
                        # Parser les artistes multiples
                        artists_data = self.parse_artists(artiste or '')
                        
                        # Fusionner avec les artistes feat.
                        artists_data = self.merge_artists(artists_data, feat_artists)
                        
                        entry = {
                            'classement': classement,
                            'artiste': artists_data['artiste'],
                            'artiste_2': artists_data['artiste_2'],
                            'artiste_3': artists_data['artiste_3'],
                            'artiste_4': artists_data['artiste_4'],
                            'titre': titre_propre,
                            'editeur': editeur or '',
                            'annee': annee,
                            'semaine': semaine
                        }
                        data.append(entry)
                        logger.debug(f"EntrÃ©e ajoutÃ©e : {entry}")
                    
                except Exception as e:
                    logger.error(f"Erreur lors de l'extraction d'un item : {e}")
                    continue
            
            # Si aucun item n'a Ã©tÃ© trouvÃ© avec la mÃ©thode structurÃ©e,
            # essayer une extraction basÃ©e sur le texte
            if len(data) == 0:
                logger.info("Tentative d'extraction alternative basÃ©e sur le texte...")
                data = self.extract_data_from_text(soup, semaine, annee)
            
            logger.info(f"Nombre d'entrÃ©es extraites : {len(data)}")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des donnÃ©es : {e}")
        
        return data
    
    def extract_data_from_text(self, soup, semaine, annee):
        """
        MÃ©thode alternative pour extraire les donnÃ©es basÃ©e sur l'analyse du texte
        """
        data = []
        
        try:
            # Obtenir tout le texte de la page
            text_content = soup.get_text()
            
            # Diviser en lignes et nettoyer
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
            
            # Pattern pour identifier un numÃ©ro de classement
            classement_pattern = re.compile(r'^(\d{1,3})$')
            
            i = 0
            while i < len(lines):
                # Chercher un numÃ©ro de classement
                if classement_pattern.match(lines[i]):
                    classement = lines[i]
                    
                    # Les lignes suivantes devraient Ãªtre titre, artiste, Ã©diteur
                    titre = None
                    artiste = None
                    editeur = None
                    
                    j = i + 1
                    collected_lines = []
                    
                    # Collecter les prochaines lignes jusqu'au prochain classement ou indicateur
                    while j < len(lines) and not classement_pattern.match(lines[j]):
                        line = lines[j]
                        # Ignorer les lignes de navigation et mÃ©tadonnÃ©es
                        if not any(skip in line.lower() for skip in ['semaine', 'nouveau', 'tÃ©lÃ©charger', 'pdf', 'prÃ©cÃ©dente', 'suivante']):
                            # Ignorer aussi les positions de la semaine derniÃ¨re
                            if not re.match(r'^\d+e?La Semaine', line, re.I):
                                collected_lines.append(line)
                        j += 1
                    
                    # Assigner les lignes collectÃ©es
                    if len(collected_lines) >= 1:
                        titre = collected_lines[0]
                    if len(collected_lines) >= 2:
                        artiste = collected_lines[1]
                    if len(collected_lines) >= 3:
                        editeur = collected_lines[2]
                    
                    # Ajouter l'entrÃ©e si on a au moins un titre
                    if titre:
                        # Nettoyer le titre et extraire les feat.
                        titre_propre, feat_artists = self.clean_title_and_extract_feat(titre)
                        
                        # Parser les artistes multiples
                        artists_data = self.parse_artists(artiste or '')
                        
                        # Fusionner avec les artistes feat.
                        artists_data = self.merge_artists(artists_data, feat_artists)
                        
                        entry = {
                            'classement': classement,
                            'artiste': artists_data['artiste'],
                            'artiste_2': artists_data['artiste_2'],
                            'artiste_3': artists_data['artiste_3'],
                            'artiste_4': artists_data['artiste_4'],
                            'titre': titre_propre,
                            'editeur': editeur or '',
                            'annee': annee,
                            'semaine': semaine
                        }
                        data.append(entry)
                    
                    i = j
                else:
                    i += 1
        
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction alternative : {e}")
        
        return data
    
    def save_to_csv(self, data, annee):
        """
        Sauvegarde les donnÃ©es dans un fichier CSV
        
        Args:
            data: Liste de dictionnaires contenant les donnÃ©es
            annee: AnnÃ©e pour le nom du fichier
        """
        if not data:
            logger.warning(f"Aucune donnÃ©e Ã  sauvegarder pour l'annÃ©e {annee}")
            return
        
        filename = os.path.join(self.data_dir, f"top_singles_{annee}.csv")
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['classement', 'artiste', 'artiste_2', 'artiste_3', 'artiste_4', 'titre', 'editeur', 'annee', 'semaine']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for row in data:
                    writer.writerow(row)
            
            logger.info(f"DonnÃ©es sauvegardÃ©es dans {filename} ({len(data)} entrÃ©es)")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du CSV : {e}")
    
    def scrape_week(self, annee, semaine):
        """
        Scrape une semaine spÃ©cifique et retourne les donnÃ©es (sans sauvegarder en CSV)
        
        Args:
            annee: AnnÃ©e Ã  scraper
            semaine: Semaine Ã  scraper
            
        Returns:
            Liste de dictionnaires contenant les donnÃ©es
        """
        logger.info(f"RÃ©cupÃ©ration des donnÃ©es : AnnÃ©e {annee}, Semaine {semaine}")
        
        soup = self.get_page_content(semaine, annee)
        if not soup:
            logger.warning(f"âœ— AnnÃ©e {annee}, Semaine {semaine} : Aucune donnÃ©e trouvÃ©e (Erreur requÃªte)")
            return []
            
        data = self.extract_data_from_page(soup, semaine, annee)
        
        if data:
            logger.info(f"âœ“ AnnÃ©e {annee}, Semaine {semaine} : {len(data)} entrÃ©es rÃ©cupÃ©rÃ©es")
            return data
        else:
            logger.warning(f"âœ— AnnÃ©e {annee}, Semaine {semaine} : Aucune donnÃ©e trouvÃ©e")
            return []

    def scrape_year(self, annee, semaine_debut, semaine_fin):
        """
        Scrape toutes les semaines d'une annÃ©e
        
        Args:
            annee: AnnÃ©e Ã  scraper
            semaine_debut: PremiÃ¨re semaine Ã  scraper
            semaine_fin: DerniÃ¨re semaine Ã  scraper
        """
        logger.info(f"DÃ©but du scraping pour l'annÃ©e {annee} (semaines {semaine_debut} Ã  {semaine_fin})")
        all_data = []
        semaines_manquantes = []
        
        for semaine in range(semaine_debut, semaine_fin + 1):
            data = self.scrape_week(annee, semaine)
            
            if data:
                all_data.extend(data)
            else:
                semaines_manquantes.append(semaine)
            
            # Pause entre les requÃªtes
            time.sleep(self.delay)
        
        # Sauvegarder toutes les donnÃ©es de l'annÃ©e
        if all_data:
            self.save_to_csv(all_data, annee)
        
        # Log des semaines manquantes
        if semaines_manquantes:
            logger.warning(f"Semaines manquantes pour {annee}: {semaines_manquantes}")
        
        return all_data
    
    def clean_existing_csv_files(self):
        """
        Supprime les fichiers CSV existants pour les rÃ©gÃ©nÃ©rer
        """
        years = [2020, 2021, 2022, 2023, 2024, 2025]
        deleted_files = []
        
        for year in years:
            filename = os.path.join(self.data_dir, f"top_singles_{year}.csv")
            if os.path.exists(filename):
                try:
                    os.remove(filename)
                    deleted_files.append(filename)
                    logger.info(f"Fichier supprimÃ© : {filename}")
                except Exception as e:
                    logger.error(f"Erreur lors de la suppression de {filename} : {e}")
        
        if deleted_files:
            logger.info(f"Suppression terminÃ©e : {len(deleted_files)} fichiers supprimÃ©s")
        else:
            logger.info("Aucun fichier CSV existant Ã  supprimer")
    
    def run(self):
        """
        Lance le scraping complet
        """
        logger.info("=" * 50)
        logger.info("DÃ©marrage du scraper SNEP")
        logger.info("=" * 50)
        
        # NOTE: Suppression automatique dÃ©sactivÃ©e pour Ã©viter la perte de donnÃ©es
        # self.clean_existing_csv_files()
        
        # Calcul dynamique de la semaine actuelle
        now = datetime.now()
        current_year = now.year
        current_week = now.isocalendar()[1]
        
        # Scraper l'annÃ©e en cours (2025)
        if current_year == 2025:
            # On va jusqu'Ã  la semaine prÃ©cÃ©dente pour Ãªtre sÃ»r que les donnÃ©es sont publiÃ©es
            # Ou jusqu'Ã  la semaine actuelle si on veut tenter
            limit_week = current_week - 1 if current_week > 1 else 1
            
            # Pour le test demandÃ©, on force jusqu'Ã  45 si on est au-delÃ , ou on utilise la logique dynamique
            # Ici je respecte la demande explicite de l'utilisateur d'aller jusqu'Ã  45 pour le test
            limit_week = 45 
            
            logger.info(f"Scraping de l'annÃ©e en cours {current_year} jusqu'Ã  la semaine {limit_week}")
            data_2025 = self.scrape_year(2025, 1, limit_week)
            logger.info(f"Total 2025 : {len(data_2025)} entrÃ©es")
        
        # Scraper les annÃ©es prÃ©cÃ©dentes seulement si nÃ©cessaire
        for year in range(2024, 2019, -1):
            # On force le scraping mÃªme si le fichier existe car on veut corriger les classements
            logger.info(f"Lancement du scraping pour {year}...")
            data_year = self.scrape_year(year, 1, 52)
            logger.info(f"Total {year} : {len(data_year)} entrÃ©es")
        
        logger.info("=" * 50)
        logger.info("Scraping terminÃ© !")
        logger.info("=" * 50)


def main():
    """
    Fonction principale
    """
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     SNEP Top Singles Scraper            â•‘
    â•‘     RÃ©cupÃ©ration des donnÃ©es en cours... â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # CrÃ©er et lancer le scraper
    scraper = SNEPScraper(delay_between_requests=1.5)
    
    try:
        scraper.run()
        print("\nâœ… Scraping terminÃ© avec succÃ¨s !")
        print("ğŸ“ Les fichiers CSV ont Ã©tÃ© sauvegardÃ©s dans le dossier 'data'")
    except KeyboardInterrupt:
        print("\nâš ï¸ Scraping interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur lors du scraping : {e}")
        logger.error(f"Erreur fatale : {e}", exc_info=True)


if __name__ == "__main__":
    main()